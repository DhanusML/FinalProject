\documentclass{beamer}

%\usetheme{Berkeley}
\usetheme{CambridgeUS}
%\usetheme{AnnArbor}
\input{/home/dhanus/Documents/latex-headers/header_presentation.tex}
\title{Sparse Recovery}
\author{Dhanus M Lal}
\date{\today}


\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Introduction}
	\begin{itemize}
		\item 
			The number of linear measurements required to completely
			understand a signal in $\R^n$ is $n$.
		
		\pause

		\item 
			If it is known that all but the first coordinate
			of it is zero, only one measurement, $y = \ip{e_1}{x}$ is 
			required.
	\pause	
		\item 
			If we know that at most one coordinate is non-zero,
			can we learn the signal with fewer than $n$ measurements?
		
		\pause

		\item Surprising result is that such a signal can be
			recovered in $\mathcal{O}(\log n)$ measurements
	\end{itemize}

\pause 

	\begin{definition}[Sparsity]
		The number of non-zero coordinates of a signal in $\R^n$.
		Denoted by $\pnorm{x}{0}$.
	\end{definition}
\end{frame}


\begin{frame}{Sparse Recovery Problem}
	\begin{itemize}
		\item Recover $x$ from linear measurement $y = Ax$ with
			$x$ being $s$ -- sparse and $A\in \R^{m\times n}$ known.

			\pause

		\item Is there a measurement matrix $A\in\R^{m\times n}$
			that makes recovery possible?

			\pause

		\item Answer is yes!
	\end{itemize}

	\pause

	\begin{proposition}
		If $A\in \R^{m\times n}$ has $m>2s$ and every subset of $2s$
		columns of $A$ is linearly independent, then sparse recovery
		problem has a unique solution.
	\end{proposition}
\pause 

	\begin{proof}
		If $x_1$ and $x_2$ are two $s$--sparse vectors with
		$Ax_1 = Ax_2$, then $A(x_1-x_2) = 0$. Now, $x_1\neq x_2$ would
		mean that a subset of $2s$ columns is linearly dependent.
	\end{proof}
\end{frame}


\begin{frame}{Basis Pursuit}
	\begin{itemize}
		\item If solution is unique, the following optimization problem
			gives the actual signal.
			\[
				x = \arg\min\pnorm{x'}{0}\quad \text{s.t } Ax' = y
			\]
			
			\pause

		\item Solving this is computationally hard: requires
			solving $\binom{n}{s}$ linear systems.

			\pause

		\item Solve the convex relaxation instead
			\[
				\hat{x} = \arg\min\pnorm{x'}{1}\quad \text{s.t } Ax' = y
			\]

			\pause

		\item This is a convex optimization problem called
			basis pursuit, it is computationally
			efficient.

			\pause

		\item Questions:
			\begin{itemize}
				\item How close is $\hat{x}$ to $x$?					
				\item Exact recovery when?
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Recovery Using Random Matrices}
	\begin{definition}
		$\mathcal{A}(m, n)$ is the class of random matrices with
		isotropic, sub-gaussian, independent rows.
	\end{definition}
	
	\pause
	Several useful properties of $\mathcal{A}(m, n)$ are known.
	
	\pause

	\begin{theorem}
		If $A\in\mathcal{A}(m, n)$ and $E = A^{-1}(z)$,
		then for any subset $T$ of $\R^n$,
		\[
			\E\diam(T\cap E)\leq Cw(T)/\sqrt{m}
		\]
		%\[
		%	\E\sup_{x\in T} \abs{\pnorm{Ax}{2}-\sqrt{m}\pnorm{x}{2}}
		%	\leq C\gamma(T)
		%\]
	\end{theorem}

	\pause

%	\begin{itemize}
		 This can be used to get
			\[
				\E\pnorm{x - \hat{x}}{2}\leq C\sqrt{\frac{{s\log n}}
				{{m}}}\pnorm{x}{2}
			\]
			

%	\end{itemize}
\end{frame}

\begin{frame}{Exact Recovery for $A\in\mathcal{A}$}
	Another property of matrices in $\mathcal{A}$.
	\begin{theorem}
		If $A\in\mathcal{A}(m, n)$ with $m\geq Cw(T)^2$, for
		$T\subset S^{n-1}$, then $T\cap \ker(A) = \emptyset$
		with high probability ($\geq 1-2\exp(-cm)$)
	\end{theorem}

	\pause

	\begin{itemize}
		\item From theorem, $\hat{x} = x$ with high
			probability provided $m>Cs\log n$.

		\item Because, if $h = \hat{x}-x\neq 0$, then
			$h/\pnorm{h}{2}\in S^{n-1}\cap 2\sqrt{s}B_1^n$, which
			is empty with high probability.

			\pause

		\item Exact recovery using basis pursuit is possible!
	\end{itemize}
\end{frame}


\begin{frame}{RIP Implies Exact Recovery}
	\begin{definition}[RIP]
		A matrix $A$ satisfies \mbox{RIP} with parameter $s$
		if the following holds for every $3s$--sparse vectors:
		\[
			0.9\pnorm{x}{2}\leq \pnorm{Ax}{2}\leq 1.1\pnorm{x}{2}
		\]
	\end{definition}
	\pause
	
	\begin{itemize}
		\item \mbox{RIP} implies exact recovery for $s$--sparse
			vectors. Proof of this only involves triangle inequality
			and Cauchy-Schwarz inequality.

			\pause

		\item All $m\times 3s$ sub-matrices must be almost isometry:
			difficult condition.

			\pause

		\item Surprisingly, $A\in\mathcal{A}(m, n)$
			satisfies \mbox{RIP}
			if $m>Cs\log n$ with high probability
			($\geq 1-2\exp(-cm))$.

		\item Alternate proof of the previous result.

	\end{itemize}
			
\end{frame}
	
\begin{frame}{Further Reading}
	\begin{itemize}
		\item Recovery of sparse signal using partial Fourier
			coefficients.
	
			\vspace{10mm}
			\pause

		\item Low rank matrix recovery

			\vspace{10mm}
			\pause

		\item Numerical simulations.
	\end{itemize}
\end{frame}
\end{document}
