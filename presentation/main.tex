\documentclass{beamer}
%\usetheme{Dresden}
%\usecolortheme{crane}
%\setbeamercolor{itemize item}{fg=darkred!80!black}
%\usetheme{Berkeley}
%\usetheme{CambridgeUS}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{JuanLesPins}
\usetheme{Rochester}
%\usetheme{Berlin}
\input{/home/dhanus/Documents/latex-headers/header_presentation.tex}
\title{Sparse Recovery}
\author{Dhanus M Lal}
\date{\today}


\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Introduction}
	\begin{itemize}
		\item 
			The number of linear measurements required to completely
			understand a signal in $\R^n$ is $n$.
		
		\pause

		\item 
			If it is known that all but the first coordinate
			of it is zero, only one measurement, $y = \ip{e_1}{x}$ is 
			required.
	\pause	
		\item 
			If we know that at most one coordinate is non-zero,
			can we learn the signal with fewer than $n$ measurements?
		
		\pause

		\item Interesting result is that such a signal can be
			recovered in $\mathcal{O}(\log n)$ measurements.
	\end{itemize}

\pause 

	\begin{definition}[Sparsity]
		The number of non-zero coordinates of a signal in $\R^n$.
		Denoted by $\pnorm{x}{0}$.
	\end{definition}
\end{frame}


\begin{frame}{Sparse recovery problem}
	\begin{itemize}
		\item Recover $s$--sparse signal $x$ from linear measurement
			$y = Ax$, where $A\in \R^{m\times n}$ known.

			\pause

		\item Are there measurement matrices $A\in\R^{m\times n}$
			that make recovery possible?

			\pause

		\item Answer is yes!
	\end{itemize}

	\pause

	\begin{proposition}
		If $A\in \R^{m\times n}$ has $m>2s$ and every subset of $2s$
		columns of $A$ is linearly independent, then sparse recovery
		problem has a unique solution.
	\end{proposition}
\pause 

	\begin{proof}
		If $x_1$ and $x_2$ are two $s$--sparse vectors with
		$Ax_1 = Ax_2$, then $A(x_1-x_2) = 0$. Now, $x_1\neq x_2$ would
		mean that a subset of $2s$ columns is linearly dependent.
	\end{proof}
\end{frame}


\begin{frame}{Basis pursuit}
	\begin{itemize}
		\item If solution is unique, the following optimization problem
			gives the actual signal.
			\[
				x = \arg\min\pnorm{x'}{0}\quad \text{s.t } Ax' = y
			\]
			
			\pause

		\item Solving this is computationally hard: requires
			solving $\binom{n}{s}$ linear systems.

			\pause

		\item Solve the convex relaxation instead
			\[
				\hat{x} = \arg\min\pnorm{x'}{1}\quad \text{s.t } Ax' = y
			\]

			\pause

		\item This is a convex optimization problem called
			\textcolor{red}{basis pursuit}, it is computationally
			efficient.

			\pause

		\item Questions:
			\begin{itemize}
				\item How close is $\hat{x}$ to $x$?					
				\item Exact recovery when?
			\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Recovery using random matrices}{The class $\mathcal{A}$}
	We will now sample measurement matrices from the class
	$\mathcal{A}(m, n)$
	\begin{definition}
		$\mathcal{A}(m, n)$ is the class of random matrices in
		$\R^{m\times n}$ with
		isotropic, sub-gaussian, independent rows.
	\end{definition}
	
\pause

	A random vector $X\in\R^n$ is:
	\begin{description}
		\item[Isotropic] if $\E XX^T = I_n$
			\pause
		\item[Sub-gaussian] if the tail of $\ip{X}{x}$ decays
			faster than some gaussian for any $x\in S^{n-1}$
	\end{description}
\end{frame}

\begin{frame}{Recovery using random matrices}{expected $\ell_2$ error}
	Several useful properties of $\mathcal{A}(m, n)$ are known.
	
	\pause

	\begin{theorem}
		Let $A\in\mathcal{A}(m, n)$, and 
		let $E = \ker(A)$, then 
		\[
			\E\diam(E\cap B_1^n)\leq C\sqrt{\frac{\log n}{m}}
		\]
	\end{theorem}

	\pause


	This can be used to get
		\[
			\E\pnorm{x - \hat{x}}{2}\leq C\sqrt{\frac{{s\log n}}
			{{m}}}\pnorm{x}{2}
		\]

\end{frame}

\begin{frame}{Exact recovery for $A\in\mathcal{A}$}
	Another property of matrices in $\mathcal{A}$.
	\begin{theorem}
		If $A\in\mathcal{A}(m, n)$ with \textcolor{red}{$m\geq cs\log n$}, then
		$\ker(A) \cap S^{n-1} \cap 2\sqrt{s}B_1^n$ is empty
		with probability at least $1-2\exp(-cm)$.
	\end{theorem}

	\pause

	\begin{itemize}
		\item From theorem, $\hat{x} = x$ with high
			probability provided $m \geq cs\log n$.

		\item Because, if $h = \hat{x}-x\neq 0$, then
			$h/\pnorm{h}{2}\in \ker(A)\cap S^{n-1}\cap 2\sqrt{s}B_1^n$, which
			is empty with high probability.

			\pause

		\item Exact recovery using basis pursuit is possible!
	\end{itemize}
\end{frame}


\begin{frame}{RIP implies exact recovery}
	\begin{definition}[RIP]
		A matrix $A$ satisfies \textcolor{red}
		{Restricted Isometry Property} with parameter $s$
		if the following holds for every $3s$--sparse vectors:
		\[
			0.9\pnorm{x}{2}\leq \pnorm{Ax}{2}\leq 1.1\pnorm{x}{2}
		\]
	\end{definition}
	\pause
	
	\begin{itemize}
		\item \mbox{RIP} implies exact recovery for $s$--sparse
			vectors. Proof of this only involves triangle inequality
			and Cauchy-Schwarz inequality.

			\pause

		\item All $m\times 3s$ sub-matrices must be almost isometry:
			difficult condition.

			\pause

		\item Surprisingly, $A\in\mathcal{A}(m, n)$
			satisfies \mbox{RIP}
			if $m\geq cs\log n$ with high probability
			($\geq 1-2\exp(-cm))$.

	\end{itemize}
			
\end{frame}
	
\begin{frame}{Summary}
	\begin{itemize}
		\item Goal: to recover sparse signal with small number of
			measurements.

			\vspace{10mm}
			\pause

		\item Basis pursuit can recover sparse signal under some
			conditions.

			\vspace{10mm}
			\pause

		\item Number of measurements depend linearly on sparsity
			and logarithmically on the dimension.
	\end{itemize}
\end{frame}


\begin{frame}{References}
\begin{thebibliography}{10}
\bibitem{hdpBook}
\alert{R. Vershynin}
\newblock  {High Dimensional Probability}
%\newblock {\em Studia Math.103. (1992), 329-333}.

\bibitem{tao}
\alert{T. Tao}
\newblock {Compressed Sensing: robust recovery of sparse signals from limited measurements}
%\newblock {\em Hokkaido Math. J., 21(2): 251-258, 1992}.
\end{thebibliography}
\end{frame}

\end{document}
